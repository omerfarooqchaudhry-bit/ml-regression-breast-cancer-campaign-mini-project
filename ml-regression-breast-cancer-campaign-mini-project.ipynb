{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Breast Cancer Campaign\n",
    "\n",
    "After the success of the second part of the campaign, the medical insurance company has become more and more into the idea of awareness-raising campaigns. However, the board needs confirmation on the company’s expenses to make sure of their financial status.\n",
    "Before you can look at the financial reports, you are required to perform regression modeling on an insurance dataset to predict the expenses based on some information about patients.\n",
    "You are required to:\n",
    "Perform EDA and visualization on the dataset.\n",
    "Implement different regression models.\n",
    "Get an acceptable R-squared score.\n",
    "\n",
    "\"\"\"\n",
    "# from networkx import display\n",
    "import sys\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import seaborn as sns\n",
    "\n",
    "    # STEP 1: Download the Dataset \n",
    "    df = pd.read_csv('insurance.csv')    \n",
    "    print(\"DATASET\", df)\n",
    "    print(\"DATASET INFO\", df.info())\n",
    "    print(\"DATASET DESCR\", df.describe())\n",
    "    print(\"DATASET DATA TYPES\", df.dtypes)\n",
    "\n",
    "\n",
    "    # STEP 2 : Reading the Dataset\n",
    "    # Step 2a: Create a boolean DataFrame showing missing values (True/False)\n",
    "    missing_values_dataframe = df.isna()\n",
    "    print(\"BOOLEAN MISSING VALUE MASK:\")\n",
    "    print(missing_values_dataframe.head())  # Shows True where data is missing\n",
    "\n",
    "    # Step 2b: Count total missing values per column\n",
    "    missing_counts = missing_values_dataframe.sum()\n",
    "    print(\"\\nCOUNT OF MISSING VALUES PER COLUMN:\")\n",
    "    print(missing_counts)\n",
    "    # Count total missing values in entire dataset\n",
    "    # This will show how many missing (NaN) values each column has — see that Unnamed: 32 has all NaNs.\n",
    "    total_missing =  missing_values_dataframe.values.sum()\n",
    "    print(\"\\nTOTAL MISSING VALUES IN DATASET:\", total_missing)\n",
    "\n",
    "    # Drop missing values\n",
    "    df = df.dropna()\n",
    "    print(\"\\nMissing values after dropping:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"DATASET\", df)\n",
    "    print(\"DATASET HEAD\",df.head())\n",
    "\n",
    "    # ----------------------------\n",
    "    # EXPLORATORY DATA ANALYSIS (EDA)\n",
    "    # ----------------------------\n",
    "    print(\"\\nDESCRIPTIVE STATS:\\n\", df.describe())\n",
    "\n",
    "\n",
    "    # Count plots for categorical features\n",
    "    for col in [\"sex\", \"smoker\", \"region\"]:\n",
    "        plt.figure()\n",
    "        sns.countplot(x=df[col])\n",
    "        plt.title(f\"Count Plot of {col}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Pair plot for a subset of features\n",
    "    plt.figure()\n",
    "    sns.pairplot(df[[\"age\", \"bmi\", \"children\", \"charges\"]])\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation matrix heat map (numeric only)\n",
    "    numeric_cols = [\"age\", \"bmi\", \"children\", \"charges\"]\n",
    "    corr = df[numeric_cols].corr()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
    "    plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Box plots for numeric features\n",
    "    for col in numeric_cols:\n",
    "        plt.figure()\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f\"Boxplot of {col}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    numeric_features = [\"age\", \"bmi\", \"children\", \"charges\"]\n",
    "    # Categorical columns\n",
    "    categorical_features = [\"sex\", \"smoker\", \"region\"]\n",
    "    \n",
    "    columns_to_encode = [\"sex\", \"smoker\", \"region\"]\n",
    "\n",
    "# Apply get_dummies() to selected columns\n",
    "    df = pd.get_dummies(df, columns=columns_to_encode, drop_first=True, dtype=int)\n",
    "    print(\"encoded_df:\\n\", df.head())\n",
    "    # encoded_df.to_csv(\"endoded_insurance.csv\", index=False)\n",
    "\n",
    "    # Combine encoded columns with the rest of numeric columns\n",
    "#     df = pd.concat(\n",
    "#         [df.drop(columns=columns_to_encode), df],\n",
    "#         axis=1\n",
    "# )\n",
    "    print(\"\\nFINAL ENCODED DF:\\n\", df.head())\n",
    "\n",
    "    # Export correct encoded CSV\n",
    "    df.to_csv(\"encoded_insurance.csv\", index=False)\n",
    "    \n",
    "    \n",
    "    # Define Features (X) and Target (y)\n",
    "    # Separating Features (X) and Target (y)\n",
    "    # Assuming 'charges' is the target\n",
    "    \n",
    "    \n",
    "    # Target & features\n",
    "    X = df.drop(\"charges\", axis=1)\n",
    "    y = df[\"charges\"]\n",
    "    \n",
    "   \n",
    "    print(\"X...\",X)\n",
    "    print(\"y...\",y)\n",
    "    \n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # df.to_csv(\"forecast_data_scaled.csv\", index=False)\n",
    "    # Convert the NumPy array to a Pandas DataFrame\n",
    "    print(\"TYPE\", type( X_scaled))\n",
    "    X_scaled_df = pd.DataFrame(X_scaled)\n",
    "    print(\"X_scaled_df\", X_scaled_df)\n",
    "\n",
    "    # Splitting the Data\n",
    "    # We split the data into 80% training, 10% validation, and 10% test sets.     \n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # First split: 80% Train, 20% Temp\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    # Second split: Split the 20% Temp into 50/50 (10% Val, 10% Test)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1)\n",
    "\n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "    modelTree = DecisionTreeRegressor(max_leaf_nodes = 10, max_depth=6).fit(X_train, y_train)\n",
    "    y_pred = modelTree.predict(X_test)\n",
    "\n",
    "    scoreDT = modelTree.score(X_test, y_test)\n",
    "    print(\"scoreDT\", scoreDT)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    y_pred = modelTree.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"MSE\", mse)\n",
    "\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    y_pred = modelTree.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(\"MAE\", mae)\n",
    "\n",
    "    \"\"\"\n",
    "    For Decision Trees:\n",
    "    The primary criterion to optimize is often tree complexity, which can be controlled by parameters like:\n",
    "    max_depth: The maximum depth of the tree.\n",
    "    min_samples_split: The minimum number of samples required to split an internal node. \n",
    "    min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "    ccp_alpha (cost-complexity pruning parameter): A parameter used for pruning, where larger values lead to more pruning.\n",
    "        \n",
    "    Cross-validation process for Decision Trees:\n",
    "    Define a range of values for the hyperparameters you want to optimize (e.g., max_depth from 1 to 20, or a range of ccp_alpha values).\n",
    "    Perform k-fold cross-validation: Split the training data into k folds. For each fold, train a Decision Tree model on k-1 folds and evaluate its performance on the remaining fold (the validation set).\n",
    "    Evaluate performance: Use a suitable metric (e.g., accuracy, F1-score, RMSE, R-squared) to assess the model's performance on the validation set for each hyperparameter combination.\n",
    "    Average the performance: Calculate the average performance across all k folds for each hyperparameter combination.\n",
    "    Select the optimal values: Choose the hyperparameter values that result in the best average performance.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "    best_score = -999\n",
    "    best_params = {}\n",
    "\n",
    "    max_depth_list = [3, 5, 7, 10, None]\n",
    "    min_samples_split_list = [2, 5, 10]\n",
    "    min_samples_leaf_list = [1, 2, 4]\n",
    "\n",
    "    print(\"\\n>>> Decision Tree Parameter Optimization <<<\\n\")\n",
    "\n",
    "    for depth in max_depth_list:\n",
    "        for min_split in min_samples_split_list:\n",
    "            for min_leaf in min_samples_leaf_list:\n",
    "                \n",
    "                modelTree = DecisionTreeRegressor(\n",
    "                    max_depth=depth,\n",
    "                    min_samples_split=min_split,\n",
    "                    min_samples_leaf=min_leaf,\n",
    "                    random_state=1\n",
    "                ).fit(X_train, y_train)\n",
    "\n",
    "                y_pred = modelTree.predict(X_test)\n",
    "                scoreDT = modelTree.score(X_test, y_test)\n",
    "\n",
    "                print(\n",
    "                    f\"max_depth={depth}, min_samples_split={min_split}, \"\n",
    "                    f\"min_samples_leaf={min_leaf} → R2={scoreDT:.3f}\"\n",
    "                )\n",
    "\n",
    "                if scoreDT > best_score:\n",
    "                    best_score = scoreDT\n",
    "                    best_params = {\n",
    "                        \"max_depth\": depth,\n",
    "                        \"min_samples_split\": min_split,\n",
    "                        \"min_samples_leaf\": min_leaf\n",
    "                    }\n",
    "\n",
    "    print(\"\\nBest Decision Tree Score:\", round(best_score, 4))\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "\n",
    "    # Retrain best model\n",
    "    best_dt = DecisionTreeRegressor(\n",
    "        **best_params,\n",
    "        random_state=1\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_dt.predict(X_test)\n",
    "\n",
    "    print(\"\\nFinal Decision Tree Evaluation:\")\n",
    "    print(\"R2:\", best_dt.score(X_test, y_test))\n",
    "    print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "    print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "    \n",
    "    \"\"\"    \n",
    "    For Random Forests:\n",
    "    Random Forests, being an ensemble of Decision Trees, have additional hyperparameters to optimize, including:\n",
    "    n_estimators: The number of trees in the forest.\n",
    "    max_features: The number of features to consider when looking for the best split.\n",
    "    Tree-specific parameters: The same parameters as for individual Decision Trees (max_depth, min_samples_split, min_samples_leaf, etc.).\n",
    "    Cross-validation process for Random Forests:\n",
    "    The process is similar to Decision Trees, but you will be optimizing a larger set of hyperparameters.\n",
    "    Define a grid of hyperparameter combinations: to explore (e.g., combinations of n_estimators, max_features, and tree-specific parameters).\n",
    "    Perform k-fold cross-validation: for each combination, training a Random Forest model and evaluating its performance on the validation sets.\n",
    "    Evaluate and average performance: using appropriate metrics.\n",
    "    Select the optimal combination: that yields the best average performance.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Random forest\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    score=[]\n",
    "    estimator_num=[]\n",
    "    temp_score = []\n",
    "\n",
    "    for estimators in range(1,301,25):\n",
    "        \n",
    "        estimator_num.append(estimators)\n",
    "        modelRandFor = RandomForestRegressor(n_estimators = estimators, \n",
    "                                            random_state = 0,\n",
    "                                            max_leaf_nodes=10).fit(X_train,\n",
    "                                                                y_train)\n",
    "        y_predRF = modelRandFor.predict(X_test)\n",
    "        \n",
    "        score = round(modelRandFor.score(X_test, y_test), 3)\n",
    "        temp_score.append(score)\n",
    "\n",
    "        print(estimators,'estimators gives a score of:',modelRandFor.score(X_test, y_test))\n",
    "\n",
    "    scoreRF = max(temp_score)\n",
    "    print('Random Forest Maximum Score is:',scoreRF)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    y_pred = modelRandFor.predict(X_test)\n",
    "    mse_randforest = mean_squared_error(y_test, y_pred)\n",
    "    print(\"MSE RANDOM FOREST\", mse_randforest)\n",
    "\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    y_pred = modelTree.predict(X_test)\n",
    "    mae_randforest = mean_absolute_error(y_test, y_pred)  \n",
    "    print(\"MAE RANDOM FOREST\", mse_randforest)\n",
    "\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "    best_score_RF = -999\n",
    "    best_params_RF = {}\n",
    "\n",
    "    # Your original estimator loop — extended for more parameters\n",
    "    for estimators in range(1, 301, 25):            # n_estimators\n",
    "        for max_feat in [\"sqrt\", \"log2\", None]:   # max_features\n",
    "            for depth in [None, 5, 10, 20]:          # max_depth\n",
    "                for min_split in [2, 5, 10]:         # min_samples_split\n",
    "                    for min_leaf in [1, 2, 4]:       # min_samples_leaf\n",
    "\n",
    "                        modelRandFor = RandomForestRegressor(\n",
    "                            n_estimators=estimators,\n",
    "                            max_features=max_feat,\n",
    "                            max_depth=depth,\n",
    "                            min_samples_split=min_split,\n",
    "                            min_samples_leaf=min_leaf,\n",
    "                            random_state=0,\n",
    "                            n_jobs=-1\n",
    "                        ).fit(X_train, y_train)\n",
    "\n",
    "                        y_predRF = modelRandFor.predict(X_test)\n",
    "                        score = round(modelRandFor.score(X_test, y_test), 4)\n",
    "\n",
    "                        print(\n",
    "                            f\"Estimators={estimators}, max_features={max_feat}, \"\n",
    "                            f\"max_depth={depth}, min_split={min_split}, \"\n",
    "                            f\"min_leaf={min_leaf} → R2={score}\"\n",
    "                        )\n",
    "\n",
    "                        # Track best model\n",
    "                        if score > best_score_RF:\n",
    "                            best_score_RF = score\n",
    "                            best_params_RF = {\n",
    "                                \"n_estimators\": estimators,\n",
    "                                \"max_features\": max_feat,\n",
    "                                \"max_depth\": depth,\n",
    "                                \"min_samples_split\": min_split,\n",
    "                                \"min_samples_leaf\": min_leaf\n",
    "                            }\n",
    "\n",
    "    print(\"\\nBEST RANDOM FOREST R2:\", best_score_RF)\n",
    "    print(\"BEST RANDOM FOREST PARAMETERS:\", best_params_RF)\n",
    "\n",
    "    # Retrain best model\n",
    "    best_rf = RandomForestRegressor(\n",
    "        **best_params_RF,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    print(\"\\nFinal Random Forest Evaluation:\")\n",
    "    print(\"R2:\", best_rf.score(X_test, y_test))\n",
    "    print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "    print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "    \n",
    "    #  - Try the SVR linear regression with a subset since it is choking\n",
    "    from sklearn.svm import SVR\n",
    "\n",
    "    modelSVR = SVR(kernel='linear', verbose=True).fit(X_train, y_train)\n",
    "    y_pred = modelSVR.predict(X_test)\n",
    "\n",
    "    scoreSVR = modelSVR.score(X_test, y_test)\n",
    "    print('Linear SVR Model gives a score of: ',scoreSVR)\n",
    "\n",
    "    sys.stdout.flush()\n",
    "    print(\">>> Moving on to SVR Polynomial SVR Model section...\")\n",
    "\n",
    "    deg = list(range(1,11))\n",
    "    scoresSVRpoly = []\n",
    "\n",
    "    for i in deg:\n",
    "        modelSVRpoly = SVR(kernel='poly',degree=i, gamma='auto', tol = 0.001, max_iter = 100000).fit(X_train, y_train)\n",
    "\n",
    "        score = modelSVRpoly.score(X_test, y_test)\n",
    "        scoresSVRpoly.append(score)\n",
    "        print(i, 'gives a score of: ',score) \n",
    "\n",
    "        scoreSVRpoly = max(scoresSVRpoly)\n",
    "        bestSVRPoly = scoresSVRpoly.index(max(scoresSVRpoly)) + 1 # location of max score\n",
    "\n",
    "        print('SVR Polynomial Regression gives a best score of:', scoreSVRpoly, \"with a degree of\",bestSVRPoly) \n",
    "\n",
    "    \n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
